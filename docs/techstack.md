Weltgewebe Tech Stack

Der Weltgewebe Tech-Stack ist ein vollstÃ¤ndig dokumentiertes Systemprofil. Er nutzt eine moderne Web-Architektur mit SvelteKit im Frontend, PostgreSQL als Source of Truth, NATS JetStream fÃ¼r Event-Distribution, und umfangreiche Ãœberwachung sowie Sicherheits- und Kostenkonzepte. Die folgenden Abschnitte fassen alle Komponenten zusammen â€“ verstÃ¤ndlich fÃ¼r Entwickler, Auditoren und PMs, mit konkreten Vorgaben und Kennzahlen.

Frontend (SvelteKit + Qwik-Escape)
	â€¢	SvelteKit-Only: Das Frontend basiert ausschlieÃŸlich auf SvelteKit, um mit minimalem Overhead und maximaler Performance native Web-App-Features zu nutzen. ZusÃ¤tzliche Frameworks werden vermieden.
	â€¢	Qwik-Escape (A/B- oder Fast-Track): Eine optionale Qwik-Integration (â€žFast-Trackâ€œ) erlaubt reines Client-Rendering dort, wo ein messbarer ROI vorliegt (z.B. extrem hohe Traffic-Routen). A/B-Tests evaluieren den Nutzen. Erst bei signifikantem Performance-Gewinn wird die Qwik-Escape-Variante aktiviert.
	â€¢	UX-Performance: Wir messen Frontend-Performance, insbesondere Long Tasks (>50ms im Browser), da sie Ã¼ber 50â€¯% der Responsiveness-Probleme verursachen ï¿¼. Entsprechende Metriken (z.B. Anzahl Long-Running Tasks pro Seite) flieÃŸen in die Ãœberwachung ein, um Code und Third-Party-Assets zu optimieren.

Backend & Datenhaltung
	â€¢	PostgreSQL + Outbox: Alle Ã„nderungen werden in PostgreSQL als â€žSource of Truthâ€œ gespeichert. Zur zuverlÃ¤ssigen Event-Publikation nutzen wir das Transactional Outbox Pattern: DatenÃ¤nderungen und zu sendende Events werden in derselben DB-Transaktion zusammengefasst ï¿¼. Ein separater Outbox-Relay-Prozess liest aus der Outbox-Tabelle und sendet die Events an NATS. So bleibt Daten- und Event-Zustand konsistent ï¿¼.
	â€¢	NATS JetStream: FÃ¼r verteilte Events (Event-Bus) setzen wir NATS JetStream ein. JetStream bietet verteilte, persitente Streams und skalierbare Consumer-Gruppen. Mit dem prometheus-nats-exporter erfassen wir JetStream-Metriken (z.B. Consumer-Lag) in Prometheus ï¿¼. Ein existierendes Grafana-Dashboard visualisiert JetStream-Stats ï¿¼. Dadurch sehen wir RÃ¼ckstÃ¤nde (Lag) von Event-Streams und kÃ¶nnen bei Problemen reagieren.
	â€¢	Transaktionale Sicherheit: Durch Outbox und logische Replikation wird sichergestellt, dass Events nur bei erfolgreichem DB-Commit versendet werden. Dies vermeidet inkonsistente ZustÃ¤nde (siehe Outbox-Pattern ï¿¼). Je nach Umfang kann die Outbox Ã¼ber Debezium/Logical Replication implementiert werden.

Monitoring & Observability
	â€¢	Prometheus & Grafana: Infrastruktur und Anwendungen werden mit Prometheus Ã¼berwacht und in Grafana visualisiert. Kernmetriken umfassen System- und Anwendungskennzahlen (CPU, Speicher, Antwortzeiten, Latenzen). Wir definieren Dashboards fÃ¼r alle relevanten Subsystenelemente (DB, Services, NATS, Edge).
	â€¢	Long-Task-Attribution: Der Browser gibt uns Informationen zu Long-Running Tasks (Hauptthread-Blocker). Wir sammeln diese durch Real-User Monitoring (z.B. Ã¼ber PerformanceObserver oder Synthetics). Wie Studien zeigen, sind lange Tasks (>50â€¯ms) Hauptursache fÃ¼r wahrgenommenen Lag ï¿¼. Die Metriken flieÃŸen in Dashboards und Alerts ein (z.B. â€ž>10 Long-Tasks auf Landing-Pageâ€œ).
	â€¢	JetStream-Lag: Ãœber den NATS-Exporter werden JetStream-spezifische Werte (z.B. consumer lag, stream depth) erfasst ï¿¼ ï¿¼. In Grafana sehen wir, ob Event-Queues anwachsen. Alerts warnen, wenn ein Consumer hinterherhinkt.
	â€¢	Edge-Kosten: Wir messen Netzwerkmetriken und CDN-Kosten. Key-Metriken sind ausgehende Traffic-Volumina und Kosten pro Gigabyte. Monitoring umfasst auÃŸerdem HTTP/3-spezifische Stats (Caddy kann diese liefern). So sehen wir, wo hohe Egress-Kosten entstehen und optimieren ggf. Caching oder Traffic-Shaping.
	â€¢	Alert-Trigger: Alerts basieren auf SLIs (siehe SLO-Matrix weiter unten). Beispiele: â€žCPU >90â€¯% lÃ¤nger als 5â€¯minâ€œ, â€žService-Response 95%-Latency >X msâ€œ oder â€ž>10% JetStream-Nachrichten-Lagâ€œ.

Data Lifecycle & DSGVO-Compliance
	â€¢	Phasenorientierte DLM: Unsere Daten durchlaufen definierte Lebenszyklus-Phasen (Erfassung, Speicherung, Nutzung, Archivierung, LÃ¶schung) ï¿¼. In der Datenspeicherung schirmen wir personenbezogene Daten mittels Encryption und Pseudonymisierung ab, um DSGVO-Anforderungen zu erfÃ¼llen ï¿¼.
	â€¢	Daten-Pipeline: Automatisierte Pipelines klassifizieren Daten beim Import (z.B. personenbezogen oder anonym), verschlÃ¼sseln sie nach Bedarf und taggen sie mit Aufbewahrungsfristen. Die Pipelines sorgen fÃ¼r konsistente Metadaten, damit spÃ¤ter entschieden wird, was wann gelÃ¶scht wird.
	â€¢	Forget-Pipeline: Um das â€žRecht auf Vergessenwerdenâ€œ zu erfÃ¼llen, haben wir einen LÃ¶schworkflow implementiert. Nach Ablauf eines Retentionszeitraums oder auf Nutzernachfrage entfernt die Pipeline alle verbliebenen persÃ¶nlichen Daten (End-of-Lifecycle) ï¿¼. Dabei kann eine Kombination aus Soft-Delete, Datenmaskierung und finaler physischer LÃ¶schung zum Einsatz kommen. Jede LÃ¶schung wird auditfÃ¤hig protokolliert.
	â€¢	Audit & Protokollierung: Zugriffe und Ã„nderungen an sensiblen Daten werden lÃ¼ckenlos geloggt. Retentions- und LÃ¶sch-FÃ¤lle sind dokumentiert, um DSGVO-Audits zu bestehen.

Disaster Recovery
	â€¢	RegelmÃ¤ÃŸige Drills: Mindestens vierteljÃ¤hrlich fÃ¼hren wir einen DR-Drill durch ï¿¼. Dabei simulieren wir einen Totalausfall des primÃ¤ren Rechenzentrums. In jedem Drill wird unsere Infrastruktur nach definiertem RPO/RTO-Konzept in einer sauberen Umgebung neu aufgebaut.
	â€¢	Rebuild + Replay: Der Drill umfasst: (1) Neuaufbau aller Cluster (Nomad, DBs, NATS, etc.) mit Infrastruktur-as-Code, (2) Event-Replay: Verarbeitung gespeicherter Events aus der Outbox/Historie, um den Datenstand zu rekonstruieren, (3) Verifikation: Konsistenz-Checks zwischen Quellsystem und Wiederherstellung. Alle Schritte werden dokumentiert und gemessen (Recovery-Time, Datenverlust).
	â€¢	Continuous Testing: Diese Ãœbung ist Teil eines kontinuierlichen Verbesserungsprozesses. Erkenntnisse flieÃŸen in die SystemhÃ¤rtung ein (z.B. Code-Updates, Automatisierung). TestRail empfiehlt, DR-Prozesse regelmÃ¤ÃŸig zu validieren ï¿¼, damit das Team eingespielt bleibt.

Service Level Objectives (SLO) & Alerts
	â€¢	Routen-granulare SLOs: FÃ¼r jeden Haupt-Service bzw. Endpunkt definieren wir eigene SLOs (z.B. 99,9â€¯% VerfÃ¼gbarkeit pro Monat, p95-Latenz â‰¤â€¯X ms). Kritische Pfade (z.B. Buchung, Checkout) haben hÃ¶here Ziele als weniger relevante Routen. So kann z.B. die API-Route /api/checkout ein eigenes SLO â€ž99,95â€¯% bez. Erfolgsrateâ€œ erhalten.
	â€¢	Fehlerbudget-Alarmierung: Zu jedem SLO wird ein Fehlerbudget und automatische Trigger konfiguriert. Wir Ã¼berwachen z.B. â€žgÃ¼ltige vs. fehlerhafte API-Antworten pro Routeâ€œ oder â€žErfolgsrate von Calls pro Endpointâ€œ. Sinkt die SLI unter das Ziel, wird sofort ein Alert ausgelÃ¶st. Tools wie Datadog erlauben es, gruppierte SLOs zu erstellen â€“ zum Beispiel nach Route oder Traffic-Knoten â€“ und Fehlerraten granular einzusehen ï¿¼.
	â€¢	Routing-Matrix: Eine SLO-Trigger-Matrix zeigt, welcher Alarm bei Ãœberschreitung welcher Schwelle ausgelÃ¶st wird (z.B. erste Warnung bei 1â€¯% Fehlerbudget-Auslastung, Eskalation bei 5â€¯%). Diese Matrix wird routenweise gepflegt und bildet die Grundlage fÃ¼r Runbooks.

Suche (Typesense / MeiliSearch)
	â€¢	PrimÃ¤re Suche: Typesense: Als schnellere SuchlÃ¶sung setzen wir Typesense ein. Typesense bietet ultraschnelle, typos-tolerante Volltextsuche und einfache Konfiguration ï¿¼. Damit kÃ¶nnen wir Instant-Suchergebnisse und AutovervollstÃ¤ndigung gewÃ¤hrleisten.
	â€¢	Fallback: MeiliSearch: Als sekundÃ¤re Engine dient MeiliSearch. Sie Ã¼berzeugt durch entwicklerfreundliches Setup und extrem schnelle Indexierung ï¿¼. FÃ¤llt Typesense aus oder erreicht es KapazitÃ¤tsgrenzen, schalten wir automatisch auf MeiliSearch um. Beide Systeme werden laufend via Monitoring auf ihre Ressourcen- und Durchsatz-Zahlen geprÃ¼ft.
	â€¢	DX-Metriken: FÃ¼r Entwickler-Effizienz (â€žDeveloper Experienceâ€œ) tracken wir Kennzahlen wie Time-to-Market von Suchfeatures, Code-Review-Durchlaufzeiten und Einrichtungsaufwand. Diese Metriken sorgen dafÃ¼r, dass wir die Wartbarkeit und Erweiterbarkeit unserer Suche kontinuierlich verbessern kÃ¶nnen.

Kostenmanagement & KPIs
	â€¢	Lastszenarien (S1â€“S4): Zur Kostenprojektion definieren wir vier Traffic-Szenarien:
	â€¢	S1 Normalbetrieb: Standard-Traffic (Basisjahr).
	â€¢	S2 Wachstum: +50â€¯% Nutzer, saisonale Peak-Zeiten.
	â€¢	S3 Spitzenlast: z.B. â€žBlack Fridayâ€œ-Ã¤hnlicher Ansturm (2â€“3Ã— Basis).
	â€¢	S4 Extremfall: UngeschÃ¤tzter Extrem-Traffic (Worst-Case).
In einer Kosten-Tabelle modellieren wir fÃ¼r jedes Szenario Sessions/Monat und Bandbreitenbedarf und berechnen die ungefÃ¤hren Cloud-Kosten (z.B. Instanz-Stunden, Daten-Egress, Speichervolumen). Darin fÃ¼hren wir auch geschÃ¤tzte KPIs wie â‚¬ pro Session oder â‚¬ pro GB auf. Solche Einheitenwerte erlauben es, Kostenentwicklungen zu interpretieren: â€žKosten/Nutzerâ€œ ist ein aussagekrÃ¤ftiger FinOps-KPI ï¿¼ ï¿¼.
	â€¢	KPI-Metriken: Basis-KPIs sind u.a. â€žâ‚¬ pro Sessionâ€œ, â€žâ‚¬ pro App-Requestâ€œ, â€žâ‚¬ pro GB Trafficâ€œ. Studien empfehlen, Cloud-Kosten in Relation zum Traffic zu setzen (z.B. Cost per Session) ï¿¼. Wir definieren Schwellenwerte (z.B. Ziel: <â€¯â‚¬1/Session) und Ã¼berwachen Abweichungen. Die KPI-Berichte werden monatlich aktualisiert.
	â€¢	Kostenkontrolle: Neben Budget-Alerts nutzen wir Cloud Cost Monitore (z.B. Ã¼ber Grafana/Cloud-Anbieter) zur Echtzeit-Ãœberwachung. So erkennen wir Abweichungen sofort und prÃ¼fen, ob sie durch geÃ¤ndertes Nutzungsverhalten gerechtfertigt sind ï¿¼.

Infrastruktur & HochverfÃ¼gbarkeit
	â€¢	Nomad-Cluster: FÃ¼r Deployment und Orchestrierung nutzen wir HashiCorp Nomad. Nomad ermÃ¶glicht Multi-Region-Cluster fÃ¼r HochverfÃ¼gbarkeit und Rolling-Updates. Alle Services (Container, Java-Services, Batch-Jobs) laufen Ã¼ber Nomad-Jobs. Nomad ist leichtgewichtig und ersetzt schwerfÃ¤llige K8s-Setups.
	â€¢	PgBouncer: Zwischen App-Servern und PostgreSQL setzen wir einen PgBouncer-Connection-Pool ein, um Datenbankverbindungen effizient zu verwalten. So skalieren wir die Zahl gleichzeitiger Clients, ohne Postgres Ã¼bermÃ¤ÃŸig zu belasten.
	â€¢	Caddy HTTP/3: Als Frontend-Proxy verwenden wir Caddy Server. Mit Caddy 2.6+ ist HTTP/3 (QUIC) standardmÃ¤ÃŸig verfÃ¼gbar ï¿¼, was Latenzen an mobilen Clients verringert. Caddy Ã¼bernimmt TLS, Load-Balancing und kann durch Plugins leicht erweitert werden.
	â€¢	HA-Pfade: Die Infrastruktur ist redundant ausgelegt: Multi-AZ-Datenbanken, mehrfach vorhandene Nomad-Server, mehrere Netzwerk-Provider. Jede kritische Komponente hat mindestens einen Ausfalls-Backup (Active/Active-Konfiguration). Netzwerkpfade sind redundant (z.B. Multi-Region-Backbone, DNS-Round-Robin).
	â€¢	Load Shedding: Um Ãœberlastung zu vermeiden, implementieren wir Load Shedding: Bei Erreichen kritischer Auslastungsgrenzen (CPU, Queue-LÃ¤ngen) lehnen Services aktiv neue Anfragen ab (HTTP 503) und schÃ¼tzen so bereits laufende Anfragen vor Timeout ï¿¼. Auf diese Weise bleibt die VerfÃ¼gbarkeit der angenommenen Anfragen hoch, selbst wenn eingehender Traffic kurzfristig stark ansteigt. Amazon empfiehlt diesen Ansatz, um Latency-Probleme in Availability-Probleme zu wandeln: Beim Hochlastpunkt soll nur der Ãœberhang ausgestoÃŸen werden, nicht alle Anfragen ï¿¼.

Sicherheit und Compliance
	â€¢	SBOM (Software Bill of Materials): Jede neue Anwendungsversion erzeugt automatisch ein SBOM (z.B. via Syft/Trivy). Das SBOM beschreibt alle AbhÃ¤ngigkeiten. Es wird zusammen mit dem Build-Artefakt archiviert und als Attestation hinterlegt ï¿¼. Bei Deployments prÃ¼fen wir das SBOM auf bekannte Schwachstellen.
	â€¢	Artifact Signing & Attestations: Container-Images und Pakete werden signiert (z.B. mit Sigstore Cosign). Neben dem SBOM legen wir erweiterte Attestations (z.B. SLSA-Provenance) als Metadaten ab ï¿¼. So ist Herkunft und IntegritÃ¤t jedes Artefakts Ã¼berprÃ¼fbar.
	â€¢	CI/CD-Gates: Unsere Pipelines erzwingen strikte Checks: Builds mit kritischen CVEs oder fehlender Signatur werden verworfen ï¿¼. Policy-Gates (Kyverno/OPA) verhindern bei Deployment nicht-konforme Artefakte: Nur signierte Images aus genehmigten Repositories dÃ¼rfen in den Cluster gelangen ï¿¼. â€žLatestâ€œ-Tags sind verboten, stattdessen verwenden wir digest-gezÃ¤hlte Artefakte.
	â€¢	Key Rotation: Alle kryptografischen SchlÃ¼ssel (z.B. Datenbank-PasswÃ¶rter, TLS-Private Keys, JWT-Keys) werden automatisiert rotiert. Wir folgen bewÃ¤hrten Policies (z.B. Rotation mindestens alle 90 Tage) ï¿¼, um das Risiko kompromittierter Keys zu begrenzen. Auch fÃ¼r API-SchlÃ¼ssel und OAuth-Tokens gelten strenge Lebensdauern. Key-Rotation ist Teil unseres Compliance-Plans (PCI-DSS, ISO 27001 empfehlen dies ausdrÃ¼cklich ï¿¼).
	â€¢	Strikte Zugriffsverwaltung: CI/CD-Zugriffe, Secrets und Konfigurations-Ã„nderungen erfordern Multi-Faktor-Authentifizierung und Genehmigungen. Wir setzen auf Infrastructure-as-Code Reviews und manuelle Freigaben fÃ¼r kritische Ã„nderungen.
	â€¢	RegelmÃ¤ÃŸige Security-Audits: Quartalsweise fÃ¼hren wir Security- und Compliance-Audits durch (z.B. SAST-Scans, Pentests der Infrastruktur, Review von Konfigurationen). Erkannten Risiken begegnen wir unmittelbar mit Patches oder Architektur-Ã„nderungen.

Observability & Runbooks
	â€¢	Umfassendes Monitoring: Logs, Metriken und Traces sind ab Deployment Day 1 aktiv. Aggregierte Logs (z.B. Ã¼ber Loki/Elasticsearch) erlauben schnelle Fehlersuche. Wir benutzen â€žOpenTelemetryâ€œ-Standards, wo sinnvoll, um Metriken und Traces einheitlich zu erfassen. So haben Entwickler und SREs Ã¼ber Dashboards stets Einblick in Systemzustand und Nutzerinteraktionen.
	â€¢	Runbooks: FÃ¼r alle kritischen Prozesse und Incident-Typen existieren Runbooks â€“ strukturierte Schritt-fÃ¼r-Schritt-Anleitungen fÃ¼r Wiederherstellung und Fehlerbehebung ï¿¼. Das beginnt bei Onboarding-Checklisten fÃ¼r neue Teammitglieder (WocheÂ 1â€“2: SystemÃ¼berblick, Account-Setup, Dev-Umgebung) und geht bis zu Incident-Runbooks (z.B. â€žNetzwerkausfallâ€œ, â€žDatenbank-Recoveryâ€œ). Runbooks minimieren Fehler im Stresstest und sorgen fÃ¼r reproduzierbare AblÃ¤ufe.
	â€¢	Onboarding (WocheÂ 1â€“2): In den ersten zwei Wochen erhÃ¤lt jeder neue Entwickler klare Dokumentation zu Infrastruktur, Tools, Zugangsdaten und Erst-Checks (Smoke-Tests). Themen sind u.a. Code-Repo, CI/CD-Pipeline, Monitoring-Zugriff, evtl. Testumgebung-Einrichtung. Diese â€žWoche-1â€œ-Dokumente sind versioniert und werden regelmÃ¤ÃŸig aktualisiert.
	â€¢	Quartalsweise Audits: Neben Security-Audits gibt es quartalsweise auch Architektur- und Compliance-Reviews. Dabei prÃ¼fen wir z.B. DatenflÃ¼sse auf DSGVO-KonformitÃ¤t, Updates von AbhÃ¤ngigkeiten auf CVEs, oder Business-Continuity-Ãœbungen. Ergebnisse werden in HandlungsplÃ¤nen festgehalten und umgesetzt.

Quellen: Technische Muster und Best Practices stammen u.a. aus aktuellen DevOps- und SRE-LeitfÃ¤den ï¿¼ ï¿¼ ï¿¼ ï¿¼. Die Zitate verweisen auf etablierte Konzepte (Outbox-Pattern, Disaster-DR-Tests, FinOps-KPIs, CI/CD-Security).


â¸»

ðŸŒ Weltgewebe Techstack â€“ Ãœbersicht

Frontend
	â€¢	SvelteKit + TypeScript â†’ Standard, einheitliche Toolchain
	â€¢	Qwik-Escape â†’ nur route-granular via A/B/Fast-Track bei messbarem ROI (â‰¥ 10 % LCP, â‰¥ 20 % TTI, â‰¤ +25 % Opex)
	â€¢	MapLibre GL + PMTiles â†’ Karten, Prebakes, Tileset-Versionierung
	â€¢	PWA â†’ Offline-Shell, feingranulare Caches
	â€¢	Security â†’ CSP/COOP/COEP, Islands-Pattern

Backend & Realtime
	â€¢	Rust (Axum + Tokio), sqlx, OpenAPI (utoipa)
	â€¢	SSE â†’ Standard fÃ¼r Live-Feeds
	â€¢	WebSocket â†’ nur fÃ¼r echte Bidir-Flows (Chat/Kollab), Idle >30 s schlieÃŸen
	â€¢	Guards â†’ SSE keep-alive, WS Token-Bucket (10/s, Burst 20)

Persistenz & Events
	â€¢	PostgreSQL 16 + PostGIS + h3-pg = Source of Truth
	â€¢	Transactional Outbox â†’ garantiert konsistente Events
	â€¢	NATS JetStream = aktiver Distributor
	â€¢	Policies: max_age=30d, max_bytes=100GiB, dupe_window=72h
	â€¢	Alarme: RAM >350 MB/Stream, Topics >50, Consumers >200, per-Consumer lag

Suche & Cache
	â€¢	Typesense (Default)
	â€¢	MeiliSearch (Fallback bei DX-Friktion)
	â€¢	KeyDB â†’ Caches, Rate-Limits, Locks
	â€¢	DX-KPIs â†’ Index-Zeit â‰¤2 h, Tuning â‰¤4 h, No-Hits-Rate, RAM

Delivery & Edge
	â€¢	Caddy (HTTP/3) â†’ Proxy, TLS, Brotli/Zstd, immutable Assets
	â€¢	Caching â†’ SSR-HTML s-maxage=600, Tiles immutable
	â€¢	Edge-Budget:
	â€¢	30d Opex-Î” â‰¤ 10 %
	â€¢	Boost â‰¤ 25â€“30 % nur bei globalem LCP-ROI (â‰¥ 300 ms in â‰¥ 3 Regionen)
	â€¢	Auto-Rollback bei > 15 % Mehrkosten ohne â‰¥ 150 ms Gewinn

Observability & Monitoring
	â€¢	Prometheus + Grafana + Loki + Tempo
	â€¢	RUM Long-Task Attribution â†’ PerformanceObserver, Budget â‰¤ 200 ms p75/Route
	â€¢	JetStream Monitoring â†’ per-Consumer lag, redeliveries, ack_wait_exceeded
	â€¢	Dashboards â†’ Web-Vitals, API-Latenzen, Search-DX, Edge-Kosten, GIS-Interaktionen

Infrastruktur & HA
	â€¢	Nomad â†’ Orchestrierung (primÃ¤r)
	â€¢	PgBouncer â†’ Connection-Pooling (transaction mode)
	â€¢	WAL-Archiv + Repl-Slots â†’ DR-Pfad
	â€¢	Caddy HTTP/3 â†’ Entry Proxy
	â€¢	HA-Pfade â†’ Compose â†’ Nomad â†’ Swarm-Mini (Drill) â†’ K8s (nur bei massivem Scale)
	â€¢	Load Shedding â†’ HTTP 503 bei Ãœberlast statt Timeout

Security & Compliance
	â€¢	SBOM (Syft/Trivy) + cosign Attestations
	â€¢	Key Rotation â†’ ed25519 halbjÃ¤hrlich, Overlap 14 Tage
	â€¢	CI-Gates â†’ clippy -D, audit/deny, Semgrep, Trivy, CodeQL
	â€¢	Access Control â†’ MFA, Secrets via sops/age
	â€¢	Data Lifecycle (DSGVO) â†’ PII-Klassen, Retention, Forget-Pipeline (Replay+Rebuild), Audit-Logs

Reliability & Governance
	â€¢	Error-Budgets â†’ 99,0â€“99,5 %/Monat; Release-Freeze bei Riss
	â€¢	Disaster-Recovery Drill â†’ vierteljÃ¤hrlich: Replica-Promote + JetStream-Replay + Outbox-Rebuild + Verify
	â€¢	Runbooks â†’ Woche 1â€“2 Onboarding + Incident Playbooks; Quartals-Audits

Kosten & KPIs
	â€¢	Traffic-Szenarien S1â€“S4: 100 â†’ 100k MAU
	â€¢	KostenbÃ¤nder: Hetzner (15â€“900 â‚¬), DO-Hybrid (70â€“2400 â‚¬)
	â€¢	KPIs: â‚¬/1 000 Sessions, â‚¬/GB egress, â‚¬/Mio Events, Edge-Quote %

â¸»

ðŸ‘‰ Kurz: mobil-first, audit-ready, rewrite-frei skalierbar.
Frontend simpel (SvelteKit-only), Events konsistent (PG Outbox + JetStream), Kosten & Latenz metrisch kontrolliert, DSGVO & Security vollstÃ¤ndig eingebaut, Disaster-Recovery geprobt.

â¸»

WELTGEWEBE TECHSTACK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Frontend
â”œâ”€ SvelteKit + TypeScript (Standard)
â”‚   â”œâ”€ MapLibre GL + PMTiles (Karten, Prebakes)
â”‚   â”œâ”€ PWA (Offline-Shell, Caches)
â”‚   â””â”€ CSP/COOP/COEP, Islands-Pattern
â””â”€ Qwik-Escape (nur bei ROI via A/B/Fast-Track)

Backend & Realtime
â”œâ”€ Rust (Axum + Tokio), sqlx, utoipa/OpenAPI
â”œâ”€ SSE (Default fÃ¼r Live-Feeds)
â””â”€ WebSocket (nur Chat/Kollab, Idle >30s Close)
   â””â”€ Guards: SSE keep-alive, WS Token-Bucket (10/s, Burst 20)

Persistenz & Events
â”œâ”€ PostgreSQL 16 + PostGIS + h3-pg (Source of Truth)
â”œâ”€ Transactional Outbox (Event-Konsistenz)
â””â”€ NATS JetStream (aktiver Distributor)
   â”œâ”€ Policies: max_age=30d, max_bytes=100GiB, dupe_window=72h
   â””â”€ Alarme: RAM >350MB/Stream, Topics >50, Consumers >200, Lag pro Consumer

Suche & Cache
â”œâ”€ Typesense (Default)
â”œâ”€ MeiliSearch (Fallback bei DX-Reibung)
â””â”€ KeyDB (Cache, Rate-Limits, Locks)

Delivery & Edge
â”œâ”€ Caddy (HTTP/3, Brotli/Zstd, immutable Assets)
â”œâ”€ Caching: SSR-HTML s-maxage=600, Tiles immutable
â””â”€ Edge-Budget:
   â”œâ”€ 30d Opex-Î” â‰¤ 10 %
   â”œâ”€ Boost â‰¤ 25â€“30 % bei globalem LCP-ROI
   â””â”€ Auto-Rollback bei >15 % Mehrkosten ohne â‰¥150ms Gewinn

Observability & Monitoring
â”œâ”€ Prometheus + Grafana + Loki + Tempo
â”œâ”€ RUM Long-Task Attribution (Budget â‰¤200ms p75/Route)
â”œâ”€ JetStream Monitoring (Lag, redeliveries, ack_wait_exceeded)
â””â”€ Dashboards: Web-Vitals, API-Latenzen, Search-DX, Edge-Kosten, GIS

Infrastruktur & HA
â”œâ”€ Nomad (Orchestrierung primÃ¤r)
â”œâ”€ PgBouncer (Connection-Pool, transaction mode)
â”œâ”€ WAL-Archiv + Repl-Slots (DR-Pfad)
â”œâ”€ Caddy HTTP/3 (Proxy)
â”œâ”€ HA-Pfade: Compose â†’ Nomad â†’ Swarm-Mini (Drill) â†’ K8s (bei Mass-Scale)
â””â”€ Load Shedding: HTTP 503 bei Ãœberlast statt Timeout

Security & Compliance
â”œâ”€ SBOM (Syft/Trivy) + cosign Attestations
â”œâ”€ Key Rotation (ed25519 halbjÃ¤hrlich, Overlap 14d)
â”œâ”€ CI-Gates: clippy -D, audit/deny, Semgrep, Trivy, CodeQL
â”œâ”€ Access Control: MFA, Secrets via sops/age
â””â”€ Data Lifecycle (DSGVO)
   â”œâ”€ PII-Klassen, Retention, Redaction
   â””â”€ Forget-Pipeline (Replay+Rebuild), Audit-Logs

Reliability & Governance
â”œâ”€ Error-Budgets: 99.0â€“99.5 % / Monat â†’ Release-Freeze bei Riss
â”œâ”€ Disaster-Recovery Drill (vierteljÃ¤hrlich)
â”‚   â””â”€ Replica-Promote + JetStream-Replay + Outbox-Rebuild + Verify
â””â”€ Runbooks
    â”œâ”€ Woche 1â€“2 Onboarding & Smoke-Tests
    â”œâ”€ Incident Playbooks (Netz, DB, API)
    â””â”€ Quartals-Audits (Security & Compliance)

Kosten & KPIs
â”œâ”€ Szenarien S1â€“S4: 100 â†’ 100k MAU
â”‚   â”œâ”€ Requests/Tag: 10k â†’ 10M
â”‚   â”œâ”€ Events/Tag:   20k â†’ 20M
â”‚   â”œâ”€ Tile-Hits:    50k â†’ 15M
â”‚   â””â”€ Volumen:      3GB â†’ 2TB
â”œâ”€ KostenbÃ¤nder:
â”‚   â”œâ”€ Hetzner:  â‚¬15â€“900
â”‚   â””â”€ DO-Hybrid: â‚¬70â€“2400
â””â”€ KPIs: â‚¬/1000 Sessions, â‚¬/GB egress, â‚¬/Mio Events, Edge-Quote %


